Introduction:
The project mainly aims to  simulates the creating  of the RAGPY system.
 RAGPY is a theoretical framework that aims to improve the quality and relevance of responses generated by AI systems through a combination of retrieval and generation techniques. By leveraging retrieved information and context,  it performs the retrieval benchmarking . It enhances the generative process to produce more accurate, coherent, and contextually relevant responses.

Prerequisites:
   Python 3.10
 Installation:
1.clone the repository :  
  ! git clone https://github.com/bayeslabs/Ragpy.git
  cd//content/Ragpy
2. Install the required packages:
    ! pip install -e .

USAGE:
Run the script main.py with  command-line argument
      !python main.py -h
 By executing the RAG pipeline. Here are the available options that you can customise with the parsers

--config: Path to the configuration file (default: ./config.yaml)
--user_files: Path(s) to the user-specified file(s) to be processed
--chunk_size: Chunk size for splitting text (default: 400)
--text_overlap: Text overlap for splitting text (default: 50)
--embedding: List of embedding options (e.g., huggingface_instruct_embeddings, all_minilm_embeddings, etc.)
--vectorstore: Vector store option (Chroma or Faiss)
--persist_dir: Path to the vector store persistent directory
--top_k: The number of top documents to be retrieved
--benchmark_data_path: Path to the benchmarking dataset in CSV with query context and ground_truth
--save_dir: Directory to save all the results
--num_questions: Number of questions to be generated in synthetic benchmarking dataset
--query: The query for which main logic is executed
--context_given: Whether or not the context is given
--model_type: The type of model to use (openai or hugging_face)
--chain_type: The type of chain to use (simple or retrieval)
--domain: The domain for which mani logic needs to be executed
--prompt_type: The type of prompt to use (general, custom, or specific)
--temperature: Temperature of the model (default: 0.7)
--llm_repo_id: Hugging face repo ID for language modelling
--db_path: Path of the database



 Example usage:
 python main.py --config ./config.yaml --user_files /path/to/files --chunk_size 400 --top_k 5

Running the Ragpy
Ensure you have the necessary API keys set up:

OPENAI_API_KEY: Your OpenAI API key
HUGGINGFACEHUB_API_TOKEN: Your Hugging Face Hub API toke

 Run RAGPY with the desired configuration:
 python main.py --config path/to/config.yaml

Configuration:
You can customize the behavior of RAGpy using the config.yaml file. Refer to the configuration file for detailed options and their descriptions.
